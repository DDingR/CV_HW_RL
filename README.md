# CV_HW_RL
영상처리 과제입니다.

# 이미지 특징 추출을 통한 강화학습

본 프로젝트는 움직이는 목표물을 투사체를 통해 맞추도록 강화학습을 적용하는데 있어서 아래 3가지 케이스를 비교하는 것을 목표로 한다.
1. 동역학적 요소 등을 포함하는 observation vector
2. 이미지에서 cv를 통해 추출한 특징으로 구성된 observation vector
3. raw RGB image pixel 

## 실험의 주요 주목점

- 강화학습의 가장 큰 도전은 사람과 같은 raw image기반으로 원하는 행동을 취하도록 학습을 하는 것이다.   (1번 케이스와 같은 동역학적 요소 등을 현실에서 정확히 측정할 수 없기 때문)
- AlexNet을 필두로 한 이미지 특징 추출 네트워크의 특이점이 오기 전까지는 대부분 수작업을 통해 이미지에서 특징을 추출하여 학습에 사용하였다. (2번 케이스와 같은)
- raw image를 사용하는 강화학습의 알고리즘들은 CNN과 같은 딥러닝 기반 이미지 추출을 사용하여 사람을 능가 할 만한 성능을 보여주었다. (3번 케이스)
- 본 프로젝트에서는 2번 케이스에 집중하여 cv를 이용해 이미지에서 특징을 추출하는 학습에 사용한다.
- 특징의 추출은 target의 변화(target의 이동), 모서리의 각도의 변화(agent의 이동) 등을 통해 할 것으로 계획 중이다.
- 결과의 분석은 다음과 같은 비교를 행할 수 있을 것으로 예상한다.
1. obercvation vector와 image기반 학습 중 어느 케이스가 성능이 더 좋은가 ( 1 vs 2, 3 )
2. cv로 추출한 특징과 이미지 추출 네트워크를 통한 특징 중 어느 케이스가 더 좋은가 ( 2 vs 3 )

## 강화학습에 대한 간단한 정보

- 강화학습은 agent(위 경우 투사체를 발사하는 player)가 자신과 주변의 환경;environment를 관찰;observation하여 현재 상태;state를 추출하고 행동;action을 취하는 과정 중에 목표(위 경우 목표물을 맞추는 행위)를 완료하여 받는 사전에 정의된 보상;reward를 최대로 만들도록 학습하는 것을 말한다. 
- 목표를 완료하거나 실패하는 경우 하나의 에피소드가 끝났다고 말한다.
- action을 취하여 agent가 다음 timestep으로 간 경우 한 step이 지났다고 말한다.
- state는 위치 정보, 동역학적 정보, reward 정보 등으로 구성 될 수 있다.
- state는 vector 형식으로 사용 할 수 있고 image와 같은 고차원 데이터를 사용할 수 도 있다.
- image의 경우 동역학적 정보를 얻기 위해 여러 장을 stack 하여 사용한다.   (시간 대비 이미지 속 물체의 변화를 통해 동역학적 정보 추출)
- 게임 속 인공지능에 적용하거나 최적제어 문제 등에 활용할 수 있다.

## 실험에 대하여

- Unity를 활용하여 environment와 agnet, action-reward 관계를 설계
- Unity에서 제공하는 python-API인 ML-agent를 통해 observation을 외부로 가져와 학습 후 action을 전송하여 Unity 상에서 시뮬레이션 진행
- 사용하는 강화학습 알고리즘은 PPO를 사용할 것으로 생각 중

## 환경에 대하여

- agent는 random으로 x, y, z 축으로 움직임
- target은 임의의 장소에 소환됨
- 투사체로 target을 맞출 경우 reward가 1 제공됨
- 10번 맞출 경우 에피소드가 종료되고 target이 다른 임의의 장소에 소환됨
- 100 step을 초과하는 경우 에피소드가 종료되고 target이 임의의 장소에 소환되고 agent가 중앙에 소환됨

## 3가지 비교 케이스에 대하여
- observation vector로 학습을 수행할 경우 DNN으로 구성된 간단한 네트워크 사용
- raw image의 경우 CNN으로 구성된 간단한 네트워크 사용



